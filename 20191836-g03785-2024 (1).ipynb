{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport time\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models, transforms\nimport torch\nimport torch.nn as nn\nimport random\nimport torch.nn.init\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tqdm import tqdm\nfrom multiprocessing import cpu_count\nfrom torch.utils.data import random_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device setting\nimport torch\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nprint('Using PyTorch version:', torch.__version__, ' device:', device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the custom dataset\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        filename = self.img_labels.iloc[idx, 0]\n        image = Image.open(os.path.join(self.img_dir, filename)).convert('RGB')\n        label = self.img_labels.iloc[idx, 1]\n\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n    def get_labels(self):\n        return self.img_labels.iloc[:, 1].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CustomSubset 클래스 정의\nclass CustomSubset(Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.subset)\n\n    def __getitem__(self, idx):\n        image, label = self.subset[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((256,256)),\n        transforms.RandomRotation(45),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transform = transforms.Compose([\n     transforms.Resize((256,256)),\n     transforms.ToTensor(),\n     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\nannotation_file = '/content/drive/MyDrive/uou-ie-g-03785-spring-2024-term-project/train.csv'\nimg_dir = '/content/drive/MyDrive/uou-ie-g-03785-spring-2024-term-project/Images/Train'\ntrain_custom = CustomImageDataset(annotation_file, img_dir)\n# train_custom = CustomImageDataset(annotation_file, img_dir, transform=train_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 전체 데이터셋의 길이\ndataset_size = len(train_custom)\n\n# 8:2 비율로 데이터셋을 분할\ntrain_size = int(dataset_size * 0.8)\nvalid_size = dataset_size - train_size\n\n# 데이터셋을 랜덤하게 분할\ntrain_data, valid_data = random_split(train_custom, [train_size, valid_size])\n\n# 데이터셋 크기 확인\nprint(f\"Train dataset size: {len(train_data)}\")\nprint(f\"Validation dataset size: {len(valid_data)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 각각의 데이터셋에 맞는 변환 적용\ntrain_dataset = CustomSubset(train_data, transform=train_transform)\nvalid_dataset = CustomSubset(valid_data, transform=test_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nannotation_file2 = '/content/drive/MyDrive/uou-ie-g-03785-spring-2024-term-project/sample_submission.csv'\nimg_dir2 = '/content/drive/MyDrive/uou-ie-g-03785-spring-2024-term-project/Images/Test'\ntest_custom = CustomImageDataset(annotation_file2, img_dir2, transform=test_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_workers = int(cpu_count() / 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, num_workers=num_workers, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, num_workers=num_workers, shuffle=False)\ntest_loader = DataLoader(test_custom, batch_size=32, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.wide_resnet101_2(pretrained=True)\nnum_ftrs = model.fc.in_features\nhalf_in_size = round(num_ftrs/2)\nlayer_width = 1024\nNum_class=4\n\nclass SpinalNet(nn.Module):\n    def __init__(self, Input_Size, Number_of_Split, HL_width, number_HL, Output_Size, Activation_Function):\n\n        super(SpinalNet, self).__init__()\n        Splitted_Input_Size = int(np.round(Input_Size/Number_of_Split))\n        self.lru = Activation_Function\n        self.fc1 = nn.Linear(Splitted_Input_Size, HL_width)\n        if number_HL>1:\n            self.fc2 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>2:\n            self.fc3 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>3:\n            self.fc4 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>4:\n            self.fc5 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>5:\n            self.fc6 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>6:\n            self.fc7 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>7:\n            self.fc8 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>8:\n            self.fc9 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>9:\n            self.fc10 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>10:\n            self.fc11 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>11:\n            self.fc12 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>12:\n            self.fc13 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>13:\n            self.fc14 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>14:\n            self.fc15 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>15:\n            self.fc16 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>16:\n            self.fc17 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>17:\n            self.fc18 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>18:\n            self.fc19 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>19:\n            self.fc20 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>20:\n            self.fc21 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>21:\n            self.fc22 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>22:\n            self.fc23 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>23:\n            self.fc24 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>24:\n            self.fc25 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>25:\n            self.fc26 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>26:\n            self.fc27 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>27:\n            self.fc28 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>28:\n            self.fc29 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n        if number_HL>29:\n            self.fc30 = nn.Linear(Splitted_Input_Size+HL_width, HL_width)\n\n        self.fcx = nn.Linear(HL_width*number_HL, Output_Size)\n\n    def forward(self, x):\n        x_all =x\n\n        Splitted_Input_Size = self.fc1.in_features\n        HL_width = self.fc2.in_features - self.fc1.in_features\n        number_HL = int(np.round(self.fcx.in_features/HL_width))\n        length_x_all = number_HL*Splitted_Input_Size\n\n        while x_all.size(dim=1) < length_x_all:\n            x_all = torch.cat([x_all, x],dim=1)\n\n        x = self.lru(self.fc1(x_all[:,0:Splitted_Input_Size]))\n        x_out = x\n\n        counter1 = 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc2(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc3(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc4(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc5(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc6(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc7(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc8(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc9(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc10(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc11(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc12(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc13(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc14(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc15(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc16(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc17(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc18(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc19(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc20(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc21(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc22(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc23(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc24(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc25(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc26(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc27(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc28(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc29(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n\n        counter1 = counter1 + 1\n        if number_HL>counter1:\n            x_from_all = x_all[:,Splitted_Input_Size* counter1:Splitted_Input_Size*(counter1+1)]\n            x = self.lru(self.fc30(torch.cat([x_from_all, x], dim=1)))\n            x_out = torch.cat([x_out, x], dim=1)\n        #print(\"Size before output layer:\",x_out.size(dim=1))\n        x = self.fcx(x_out)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fc = SpinalNet(Input_Size = num_ftrs, Number_of_Split =2, HL_width=1024, number_HL=30, Output_Size=Num_class, Activation_Function = nn.ReLU(inplace=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p][\"e_w\"] = e_w\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        p.grad.norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = [1.0, 1.0, 1.2, 1.3]\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\nclass_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nlr = 0.0001\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\nbase_optimizer = torch.optim.Adam\noptimizer = SAM(model.parameters(), base_optimizer, lr=lr)\nexp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Early stopping parameters\nearly_stopping_patience = 7\nbest_valid_loss = float('inf')\npatience_counter = 0\n\nfor epoch in range(epochs):\n    model.train()\n\n    train_loss = 0\n    train_correct = 0\n    tqdm_dataset = tqdm(train_loader)\n    for x, y in tqdm_dataset:\n        x = x.to(device)\n        y = y.to(device)\n\n        # Define the closure function\n        def closure():\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            return loss, outputs\n\n        # Perform the first step\n        loss, outputs = closure()\n        optimizer.first_step(zero_grad=True)\n\n        # Perform the second step\n        _, outputs = closure()\n        optimizer.second_step(zero_grad=True)\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        train_correct += predicted.eq(y).sum().item()\n\n        tqdm_dataset.set_postfix({\n            'Epoch': epoch + 1,\n            'Loss': '{:06f}'.format(loss.item()),\n        })\n\n    train_loss = train_loss / len(train_loader)\n    train_acc = train_correct / len(train_loader.sampler)\n\n    model.eval()\n\n    valid_loss = 0\n    valid_correct = 0\n    tqdm_dataset = tqdm(valid_loader)\n    with torch.no_grad():\n        for x, y in tqdm_dataset:\n            x = x.to(device)\n            y = y.to(device)\n\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            valid_loss += loss.item()\n            _, predicted = outputs.max(1)\n            valid_correct += predicted.eq(y).sum().item()\n\n            tqdm_dataset.set_postfix({\n                'Epoch': epoch + 1,\n                'Loss': '{:06f}'.format(loss.item()),\n            })\n\n    valid_loss = valid_loss / len(valid_loader)\n    valid_acc = valid_correct / len(valid_loader.sampler)\n\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.6f}, Valid Loss: {valid_loss:.6f}, Valid Acc: {valid_acc:.6f}')\n\n    # Move the scheduler step after optimizer step\n    exp_lr_scheduler.step(valid_loss)\n\n    # Check early stopping condition\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        patience_counter = 0\n        # Save the best model\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stopping_patience:\n            print(f'Early stopping triggered after {epoch+1} epochs.')\n            break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = 'best_model.pth'\nmodel.load_state_dict(torch.load(path))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/content/drive/MyDrive/uou-ie-g-03785-spring-2024-term-project/sample_submission.csv')\n\nmodel.eval()\n\nbatch_index = 0\n\nfor i, (images, targets) in enumerate(test_loader):\n    images = images.to(device)\n    outputs = model(images)\n    batch_index = i * 32\n    max_vals, max_indices = torch.max(outputs, 1)\n    sample_submission.iloc[batch_index:batch_index + 32, 1:] = max_indices.long().cpu().numpy()[:,np.newaxis]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('submit.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import files\n\n# submission.csv 파일 다운로드\nfiles.download('submit.csv')","metadata":{},"execution_count":null,"outputs":[]}]}